import { NextRequest, NextResponse } from 'next/server';
import { createClient } from '@supabase/supabase-js';
import { analyzeAnnouncement, normalizeDate } from '@/lib/gemini';
import { smartExtractContent, validateContentQuality, detectPageType } from '@/lib/smart-content-extractor';

// ============================================================
// Direct URL Crawler API
// ============================================================
// ëª©ì : 211ê°œ ì§ì ‘ URLì—ì„œ ë³´ì¡°ê¸ˆ ê³µê³  í¬ë¡¤ë§
// íŠ¹ì§•: Vercel Pro í˜¸í™˜ (60ì´ˆ íƒ€ì„ì•„ì›ƒ), Playwright í¬ë¡¤ë§, ë°°ì¹˜ ì²˜ë¦¬
// ============================================================

export const dynamic = 'force-dynamic';
export const runtime = 'nodejs';
export const maxDuration = 60; // Vercel Pro ìµœëŒ€ê°’: 60ì´ˆ (ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§ ê°œìˆ˜ ì œí•œìœ¼ë¡œ ëŒ€ì‘)

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!;
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY!;
const supabase = createClient(supabaseUrl, supabaseServiceKey);

const CRAWLER_SECRET = process.env.CRAWLER_SECRET || 'dev-secret';
const IS_PRODUCTION = process.env.NODE_ENV === 'production';

// ============================================================
// íƒ€ì… ì •ì˜
// ============================================================

interface DirectCrawlRequest {
  urls?: string[];              // í¬ë¡¤ë§í•  URL ëª©ë¡ (max 10)
  direct_mode: true;           // ì§ì ‘ URL ëª¨ë“œ ì‹ë³„ì
  retry_failed?: boolean;      // ì‹¤íŒ¨í•œ URLë§Œ ì¬ì‹œë„
  batch_size?: number;         // ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ 10)
}

interface CrawlResult {
  url: string;
  success: boolean;
  announcements_found?: number;
  new_announcements?: number;
  relevant_announcements?: number;
  error?: string;
}

interface DirectCrawlResponse {
  success: boolean;
  total_urls: number;
  successful_urls: number;
  failed_urls: number;
  new_announcements: number;
  relevant_announcements: number;
  results: CrawlResult[];
  errors?: string[];
  crawl_log_id?: string;
}

// ============================================================
// í•„ìˆ˜ í‚¤ì›Œë“œ ê²€ì‚¬
// ============================================================

const REQUIRED_KEYWORDS = [
  'IoT', 'iot', 'IOT', 'ì‚¬ë¬¼ì¸í„°ë„·',
  'ì†Œê·œëª¨ ëŒ€ê¸°ë°°ì¶œ', 'ì†Œê·œëª¨ëŒ€ê¸°ë°°ì¶œ', 'ì†Œê·œëª¨ ëŒ€ê¸°ì˜¤ì—¼',
  'ë°©ì§€ì‹œì„¤', 'ëŒ€ê¸°ë°©ì§€ì‹œì„¤', 'ëŒ€ê¸°ì˜¤ì—¼ë°©ì§€',
  'ëŒ€ê¸°ë°°ì¶œì‹œì„¤', 'ë°°ì¶œì‹œì„¤',
  'êµ´ëš', 'TMS', 'ìë™ì¸¡ì •', 'ì¸¡ì •ê¸°ê¸°',
  'í™˜ê²½IoT', 'ìŠ¤ë§ˆíŠ¸í™˜ê²½', 'ì›ê²©ê°ì‹œ',
];

const EXCLUDE_KEYWORDS = [
  'ì±„ìš©', 'ëª¨ì§‘', 'ì§ì›', 'ì¸ë ¥', 'êµ¬ì¸',
  'ì…ì°°', 'ë‚™ì°°', 'ê³„ì•½', 'ìš©ì—­',
  'ê²°ê³¼', 'ë°œí‘œ', 'ì„ ì •', 'í•©ê²©',
];

function hasRequiredKeyword(title: string): boolean {
  const lowerTitle = title.toLowerCase();
  return REQUIRED_KEYWORDS.some(k => lowerTitle.includes(k.toLowerCase()));
}

function hasExcludeKeyword(title: string): boolean {
  const lowerTitle = title.toLowerCase();
  return EXCLUDE_KEYWORDS.some(k => lowerTitle.includes(k.toLowerCase()));
}

function isRelevantTitle(title: string): boolean {
  return hasRequiredKeyword(title) && !hasExcludeKeyword(title);
}

// ============================================================
// ì§ì ‘ URL í¬ë¡¤ë§ í•¨ìˆ˜ (Playwright ê¸°ë°˜)
// ============================================================

async function crawlDirectUrl(url: string): Promise<{
  success: boolean;
  announcements: any[];
  error?: string;
}> {
  let browser;
  try {
    const { chromium } = await import('playwright-core');
    const chromiumPack = await import('@sparticuz/chromium');

    // ë¸Œë¼ìš°ì € ì‹¤í–‰ (Vercel Pro í˜¸í™˜)
    browser = await chromium.launch({
      args: chromiumPack.default.args,
      executablePath: await chromiumPack.default.executablePath(),
      headless: chromiumPack.default.headless,
    });

    const context = await browser.newContext({
      userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    });

    const page = await context.newPage();

    // í˜ì´ì§€ ë¡œë“œ (10ì´ˆ íƒ€ì„ì•„ì›ƒ - Vercel 60ì´ˆ ì œí•œ ê³ ë ¤)
    await page.goto(url, {
      timeout: 10000,
      waitUntil: 'domcontentloaded',
    });

    // ğŸ” í˜ì´ì§€ íƒ€ì… ê°ì§€ (í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§)
    const pageType = await detectPageType(page);
    console.log(`  ğŸ“Š í˜ì´ì§€ íƒ€ì…: ${pageType.type} (ì‹ ë¢°ë„: ${pageType.confidence.toFixed(2)})`);

    const announcements: any[] = [];

    if (pageType.type === 'list' && pageType.detailLinks && pageType.detailLinks.length > 0) {
      // ğŸ“‹ ëª©ë¡ í˜ì´ì§€: ê° ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (ìµœëŒ€ 3ê°œë¡œ ì œí•œ - Vercel 60ì´ˆ íƒ€ì„ì•„ì›ƒ ëŒ€ì‘)
      const maxDetailPages = 3;
      const limitedLinks = pageType.detailLinks.slice(0, maxDetailPages);
      console.log(`  ğŸ“‹ ëª©ë¡ í˜ì´ì§€ ê°ì§€ - ${pageType.detailLinks.length}ê°œ ë§í¬ ì¤‘ ${limitedLinks.length}ê°œ ì²˜ë¦¬`);

      for (const link of limitedLinks) {
        try {
          console.log(`  â†’ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§: ${link}`);
          await page.goto(link, { timeout: 10000, waitUntil: 'domcontentloaded' });

          // ìƒì„¸ í˜ì´ì§€ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
          const extractionResult = await smartExtractContent(page, link);
          const content = extractionResult.content.replace(/\s+/g, ' ').trim();

          // ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦
          const validation = validateContentQuality(content);
          console.log(`    ğŸ“Š í’ˆì§ˆ: ${validation.score.toFixed(2)} | ì‹ ë¢°ë„: ${extractionResult.confidence.toFixed(2)}`);

          // ì œëª© ì¶”ì¶œ (í˜ì´ì§€ íƒ€ì´í‹€ ë˜ëŠ” h1 íƒœê·¸)
          let title = await page.title();
          if (!title || title.length < 5) {
            const h1 = await page.locator('h1').first().textContent({ timeout: 1000 }).catch(() => null);
            title = h1 || 'ì œëª© ì—†ìŒ';
          }
          title = title.trim();

          // ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€ í†µê³¼ ì‹œ ì¶”ê°€
          if (content.length >= 100 && validation.score >= 0.3) {
            announcements.push({
              title,
              content,
              source_url: link,
              crawled_at: new Date().toISOString(),
            });
            console.log(`    âœ… ì¶”ê°€ ì™„ë£Œ: ${title}`);
          } else {
            console.warn(`    âš ï¸  í’ˆì§ˆ ë¯¸ë‹¬: ${validation.issues.join(', ')}`);
          }

          // Rate limiting (500ms ëŒ€ê¸°)
          await new Promise(resolve => setTimeout(resolve, 500));

        } catch (error: any) {
          console.warn(`  âš ï¸  ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: ${link} - ${error.message}`);
        }
      }

    } else {
      // ğŸ“„ ìƒì„¸ í˜ì´ì§€: ì§ì ‘ ì½˜í…ì¸  ì¶”ì¶œ
      console.log(`  ğŸ“„ ìƒì„¸ í˜ì´ì§€ ê°ì§€ - ì§ì ‘ ì¶”ì¶œ`);

      // ìŠ¤ë§ˆíŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
      const extractionResult = await smartExtractContent(page, url);
      const content = extractionResult.content.replace(/\s+/g, ' ').trim();

      // ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦
      const validation = validateContentQuality(content);
      console.log(`  ğŸ“Š í’ˆì§ˆ ì ìˆ˜: ${validation.score.toFixed(2)} (ì‹ ë¢°ë„: ${extractionResult.confidence.toFixed(2)})`);

      if (!validation.isValid) {
        console.warn(`  âš ï¸  í’ˆì§ˆ ì´ìŠˆ: ${validation.issues.join(', ')}`);
      }

      // HTML ì†ŒìŠ¤ì—ì„œ ì œëª© ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
      const html = await page.content();

      // ì œëª© ì¶”ì¶œ (a íƒœê·¸, h1-h6 ë“±)
      const titleRegex = /<(?:a[^>]*|h[1-6][^>]*)>([^<]+)<\//gi;
      const matches = html.matchAll(titleRegex);

      const seenTitles = new Set<string>();

      for (const match of matches) {
        const title = match[1].trim();

        // ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ê²€ì‚¬
        if (!seenTitles.has(title) && isRelevantTitle(title)) {
          seenTitles.add(title);
          announcements.push({
            title,
            content,
            source_url: url,
            crawled_at: new Date().toISOString(),
          });
        }
      }
    }

    await browser.close();

    console.log(`  âœ… í¬ë¡¤ë§ ì™„ë£Œ: ${announcements.length}ê°œ ê³µê³  ë°œê²¬`);

    return {
      success: true,
      announcements,
    };

  } catch (error: any) {
    if (browser) {
      await browser.close();
    }
    return {
      success: false,
      announcements: [],
      error: error.message || 'Unknown error',
    };
  }
}

// ============================================================
// Supabase ì €ì¥ í•¨ìˆ˜
// ============================================================

async function saveAnnouncements(
  announcements: any[],
  sourceUrl: string
): Promise<{ new_count: number; relevant_count: number }> {
  let newCount = 0;
  let relevantCount = 0;

  // sourceUrlì—ì„œ ì§€ìì²´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
  const { data: urlSource } = await supabase
    .from('direct_url_sources')
    .select('region_code, region_name, category')
    .eq('url', sourceUrl)
    .single();

  const regionCode = urlSource?.region_code || '00000';
  const regionName = urlSource?.region_name || 'Direct URL Source';

  for (const announcement of announcements) {
    try {
      // Gemini AI ë¶„ì„
      const analysisResult = await analyzeAnnouncement(
        announcement.title,
        announcement.content || '',
        announcement.source_url
      );

      const relevanceScore = analysisResult?.relevance_score ?? 0;
      const isRelevant = relevanceScore >= 0.75;

      if (isRelevant) {
        relevantCount++;
      }

      // ì¤‘ë³µ í™•ì¸ (source_url UNIQUE ì œì•½)
      const { data: existing } = await supabase
        .from('subsidy_announcements')
        .select('id')
        .eq('source_url', announcement.source_url)
        .single();

      if (existing) {
        continue; // ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
      }

      // Gemini AIê°€ ì¶”ì¶œí•œ ì •ë³´
      const extractedInfo = analysisResult?.extracted_info || {};

      // ë‚ ì§œ ì •ê·œí™”
      const startDate = normalizeDate(extractedInfo.application_period_start);
      const endDate = normalizeDate(extractedInfo.application_period_end);

      // ì‚½ì…
      const { error } = await supabase
        .from('subsidy_announcements')
        .insert({
          title: announcement.title,
          content: announcement.content || '',
          source_url: announcement.source_url,
          region_code: regionCode,
          region_name: regionName,
          region_type: 'basic', // Required NOT NULL field
          published_at: new Date().toISOString(),
          relevance_score: relevanceScore,
          is_relevant: isRelevant, // Set boolean flag
          crawled_at: new Date().toISOString(),
          // Gemini AI ì¶”ì¶œ ì •ë³´
          application_period_start: startDate,
          application_period_end: endDate,
          budget: extractedInfo.budget || null,
          target_description: extractedInfo.target_description || null,
          support_amount: extractedInfo.support_amount || null,
          keywords_matched: analysisResult?.keywords_matched || [],
        });

      if (error) {
        console.error('âŒ Failed to insert announcement:', announcement.title, error);
      } else {
        newCount++;
      }

    } catch (error) {
      console.error('Failed to save announcement:', error);
    }
  }

  return { new_count: newCount, relevant_count: relevantCount };
}

// ============================================================
// í¬ë¡¤ë§ ë¡œê·¸ ê¸°ë¡
// ============================================================

async function createCrawlLog(
  totalUrls: number
): Promise<string> {
  const { data, error } = await supabase
    .from('crawl_logs')
    .insert({
      crawl_type: 'direct',
      started_at: new Date().toISOString(),
      total_urls: totalUrls,
    })
    .select('id')
    .single();

  if (error || !data) {
    throw new Error('Failed to create crawl log');
  }

  return data.id;
}

async function updateCrawlLog(
  logId: string,
  results: {
    successful: number;
    failed: number;
    newAnnouncements: number;
    relevantAnnouncements: number;
    errors: string[];
  }
): Promise<void> {
  await supabase
    .from('crawl_logs')
    .update({
      completed_at: new Date().toISOString(),
      successful_urls: results.successful,
      failed_urls: results.failed,
      new_announcements: results.newAnnouncements,
      relevant_announcements: results.relevantAnnouncements,
      errors: results.errors,
    })
    .eq('id', logId);
}

// ============================================================
// direct_url_sources í…Œì´ë¸” ì—…ë°ì´íŠ¸
// ============================================================

async function recordCrawlSuccess(url: string): Promise<void> {
  const { data: func } = await supabase.rpc('record_crawl_success', {
    p_url: url,
  });
}

async function recordCrawlFailure(url: string, error: string): Promise<void> {
  const { data: func } = await supabase.rpc('record_crawl_failure', {
    p_url: url,
    p_error: error,
  });
}

// ============================================================
// GET: í¬ë¡¤ë§ ëŒ€ìƒ URL ê°€ì ¸ì˜¤ê¸°
// ============================================================

export async function GET(request: NextRequest) {
  // ì¸ì¦ í™•ì¸: CRAWLER_SECRET, Authorization Bearer, ë˜ëŠ” ì¿ í‚¤ ì„¸ì…˜
  const authHeader = request.headers.get('authorization');
  const token = authHeader?.replace('Bearer ', '');

  // 1. CRAWLER_SECRET ì¸ì¦ (GitHub Actionsìš©)
  if (token && token === CRAWLER_SECRET) {
    // GitHub Actions í¬ë¡¤ëŸ¬ ì¸ì¦ ì„±ê³µ
  }
  // 2. Authorization Bearer í† í° ì¸ì¦ (JWT ë˜ëŠ” Supabase ì„¸ì…˜)
  else if (token && token !== CRAWLER_SECRET) {
    // 2-1. JWT í† í° ê²€ì¦ ì‹œë„
    const { getUserFromToken } = await import('@/lib/secure-jwt');
    const jwtUser = await getUserFromToken(request);

    if (jwtUser) {
      // JWT í† í°ìœ¼ë¡œ ì¸ì¦ ì„±ê³µ
      if (jwtUser.permission_level < 4) {
        return NextResponse.json(
          { error: 'Forbidden: Insufficient permissions (requires level 4)' },
          { status: 403 }
        );
      }
      // JWT ì¸ì¦ ì„±ê³µ, ê³„ì† ì§„í–‰
    } else {
      // 2-2. JWT ì‹¤íŒ¨ ì‹œ Supabase ì„¸ì…˜ í™•ì¸
      const { data: { user }, error: authError } = await supabase.auth.getUser(token);

      if (authError || !user) {
        return NextResponse.json(
          { error: 'Unauthorized: Invalid token (neither JWT nor Supabase session)' },
          { status: 401 }
        );
      }

      // ì‚¬ìš©ì ê¶Œí•œ í™•ì¸ (permission_level >= 4)
      const { data: userData, error: userError } = await supabase
        .from('users')
        .select('permission_level')
        .eq('id', user.id)
        .single();

      if (userError || !userData || userData.permission_level < 4) {
        return NextResponse.json(
          { error: 'Forbidden: Insufficient permissions (requires level 4)' },
          { status: 403 }
        );
      }
    }
  }
  // 3. ì¿ í‚¤ ê¸°ë°˜ ì„¸ì…˜ ì¸ì¦ (í´ë°±)
  else {
    // ì¿ í‚¤ì—ì„œ ì„¸ì…˜ í† í° ê°€ì ¸ì˜¤ê¸°
    const cookieHeader = request.headers.get('cookie') || '';
    const cookies = Object.fromEntries(
      cookieHeader.split('; ').map(c => c.split('=').map(decodeURIComponent))
    );
    const accessToken = cookies['sb-access-token'] || cookies['sb-uvdvfsjekqshxtxthxeq-auth-token'];

    if (!accessToken) {
      return NextResponse.json(
        { error: 'Unauthorized: No session token' },
        { status: 401 }
      );
    }

    // Supabase ì„¸ì…˜ í™•ì¸
    const { data: { user }, error: authError } = await supabase.auth.getUser(accessToken);

    if (authError || !user) {
      return NextResponse.json(
        { error: 'Unauthorized: Invalid session' },
        { status: 401 }
      );
    }

    // ì‚¬ìš©ì ê¶Œí•œ í™•ì¸ (permission_level >= 4)
    const { data: userData, error: userError } = await supabase
      .from('users')
      .select('permission_level')
      .eq('id', user.id)
      .single();

    if (userError || !userData || userData.permission_level < 4) {
      return NextResponse.json(
        { error: 'Forbidden: Insufficient permissions (requires level 4)' },
        { status: 403 }
      );
    }
  }

  const { searchParams } = new URL(request.url);
  const limit = parseInt(searchParams.get('limit') || '10', 10);

  // direct_url_sourcesì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ URL ê°€ì ¸ì˜¤ê¸°
  const { data: urls, error } = await supabase.rpc('get_urls_for_crawling', {
    p_limit: limit,
  });

  if (error) {
    return NextResponse.json(
      { error: 'Failed to fetch URLs', details: error.message },
      { status: 500 }
    );
  }

  return NextResponse.json({
    success: true,
    total_urls: urls?.length || 0,
    urls: urls || [],
  });
}

// ============================================================
// POST: ì§ì ‘ URL í¬ë¡¤ë§ ì‹¤í–‰
// ============================================================

export async function POST(request: NextRequest) {
  // ì¸ì¦ í™•ì¸ (í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œë§Œ)
  if (IS_PRODUCTION) {
    const authHeader = request.headers.get('authorization');
    const token = authHeader?.replace('Bearer ', '');

    if (!token || token !== CRAWLER_SECRET) {
      return NextResponse.json(
        { error: 'Unauthorized' },
        { status: 401 }
      );
    }
  }

  try {
    const body: DirectCrawlRequest = await request.json();

    // direct_mode ê²€ì¦
    if (!body.direct_mode) {
      return NextResponse.json(
        { error: 'direct_mode must be true' },
        { status: 400 }
      );
    }

    // URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    let urlsToProcess: string[] = [];

    if (body.urls && body.urls.length > 0) {
      // ëª…ì‹œì ìœ¼ë¡œ ì œê³µëœ URL ì‚¬ìš©
      urlsToProcess = body.urls.slice(0, 10); // max 10
    } else if (body.retry_failed) {
      // ì‹¤íŒ¨í•œ URLë§Œ ì¬ì‹œë„
      const { data: failedUrls } = await supabase
        .from('direct_url_sources')
        .select('url')
        .gt('consecutive_failures', 0)
        .eq('is_active', true)
        .limit(10);

      urlsToProcess = failedUrls?.map(u => u.url) || [];
    } else {
      // DBì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ ê°€ì ¸ì˜¤ê¸°
      const { data: urls } = await supabase.rpc('get_urls_for_crawling', {
        p_limit: body.batch_size || 10,
      });

      urlsToProcess = urls?.map((u: any) => u.url) || [];
    }

    if (urlsToProcess.length === 0) {
      return NextResponse.json(
        { error: 'No URLs to process' },
        { status: 400 }
      );
    }

    // í¬ë¡¤ë§ ë¡œê·¸ ìƒì„±
    const logId = await createCrawlLog(urlsToProcess.length);

    // ë³‘ë ¬ í¬ë¡¤ë§ (max 10 URLs)
    const crawlPromises = urlsToProcess.map(url => crawlDirectUrl(url));
    const crawlResults = await Promise.all(crawlPromises);

    // ê²°ê³¼ ì§‘ê³„
    let successfulUrls = 0;
    let failedUrls = 0;
    let totalNewAnnouncements = 0;
    let totalRelevantAnnouncements = 0;
    const errors: string[] = [];
    const results: CrawlResult[] = [];

    for (let i = 0; i < urlsToProcess.length; i++) {
      const url = urlsToProcess[i];
      const result = crawlResults[i];

      if (result.success) {
        successfulUrls++;

        // Supabase ì €ì¥
        const { new_count, relevant_count } = await saveAnnouncements(
          result.announcements,
          url
        );

        totalNewAnnouncements += new_count;
        totalRelevantAnnouncements += relevant_count;

        // direct_url_sources ì—…ë°ì´íŠ¸ (ì„±ê³µ)
        await recordCrawlSuccess(url);

        results.push({
          url,
          success: true,
          announcements_found: result.announcements.length,
          new_announcements: new_count,
          relevant_announcements: relevant_count,
        });

      } else {
        failedUrls++;

        const errorMsg = result.error || 'Unknown error';
        errors.push(`${url}: ${errorMsg}`);

        // direct_url_sources ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨)
        await recordCrawlFailure(url, errorMsg);

        results.push({
          url,
          success: false,
          error: errorMsg,
        });
      }
    }

    // í¬ë¡¤ë§ ë¡œê·¸ ì—…ë°ì´íŠ¸
    await updateCrawlLog(logId, {
      successful: successfulUrls,
      failed: failedUrls,
      newAnnouncements: totalNewAnnouncements,
      relevantAnnouncements: totalRelevantAnnouncements,
      errors,
    });

    const response: DirectCrawlResponse = {
      success: true,
      total_urls: urlsToProcess.length,
      successful_urls: successfulUrls,
      failed_urls: failedUrls,
      new_announcements: totalNewAnnouncements,
      relevant_announcements: totalRelevantAnnouncements,
      results,
      errors: errors.length > 0 ? errors : undefined,
      crawl_log_id: logId,
    };

    return NextResponse.json(response);

  } catch (error: any) {
    console.error('Direct crawler error:', error);
    return NextResponse.json(
      {
        success: false,
        error: error.message || 'Internal server error',
      },
      { status: 500 }
    );
  }
}
